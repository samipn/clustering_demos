{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samipn/clustering_demos/blob/main/audio_clustering_imagebind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMLGHoOW19hs"
      },
      "source": [
        "# Assignment (i): Audio Clustering with ImageBind Embeddings\n",
        "\n",
        "This notebook uses ImageBind to extract audio embeddings, clusters them with K-Means, and evaluates clustering quality.\n",
        "\n",
        "> **Note:** You must provide audio files in `/content/audio` (or adjust the path) when running in Colab.\n"
      ],
      "id": "EMLGHoOW19hs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4gfECuj19hu",
        "outputId": "07164a81-cbcf-4b9e-a440-1f61c06f99d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet git+https://github.com/facebookresearch/ImageBind.git\n",
        "!pip install --quiet timm einops soundfile librosa torchcodec"
      ],
      "id": "y4gfECuj19hu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJim9O4O19hv",
        "outputId": "439d6139-f4a2-45a8-e31a-ceb2c5056cff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imagebind weights to .checkpoints/imagebind_huge.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.47G/4.47G [00:17<00:00, 282MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageBindModel(\n",
              "  (modality_preprocessors): ModuleDict(\n",
              "    (vision): RGBDTPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Sequential(\n",
              "          (0): PadIm2Video()\n",
              "          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "        )\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (text): TextPreprocessor(\n",
              "      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n",
              "      (mask): tensor((77, 77), requires_grad=False)\n",
              "      \n",
              "      (token_embedding): Embedding(49408, 1024)\n",
              "    )\n",
              "    (audio): AudioPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
              "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (depth): RGBDTPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 384), requires_grad=True)\n",
              "      \n",
              "      (depth_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (thermal): ThermalPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (imu): IMUPreprocessor(\n",
              "      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n",
              "      (cls_token): tensor((1, 1, 512), requires_grad=True)\n",
              "      \n",
              "      (imu_stem): PatchEmbedGeneric(\n",
              "        (proj): Linear(in_features=48, out_features=512, bias=False)\n",
              "        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (modality_trunks): ModuleDict(\n",
              "    (vision): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (12): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (13): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (14): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (15): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (16): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (17): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (18): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (19): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (20): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (21): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (22): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (23): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (24): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (25): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (26): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (27): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (28): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (29): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (30): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (31): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (text): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (12): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (13): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (14): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (15): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (16): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (17): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (18): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (19): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (20): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (21): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (22): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (23): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (audio): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.009)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.018)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.027)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.036)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.045)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.055)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.064)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.073)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.082)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.091)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.100)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (depth): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (thermal): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (imu): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.140)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.280)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.420)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.560)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.700)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "  )\n",
              "  (modality_heads): ModuleDict(\n",
              "    (vision): Sequential(\n",
              "      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
              "    )\n",
              "    (text): SelectEOSAndProject(\n",
              "      (proj): Sequential(\n",
              "        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "      )\n",
              "    )\n",
              "    (audio): Sequential(\n",
              "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
              "    )\n",
              "    (depth): Sequential(\n",
              "      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=384, out_features=1024, bias=False)\n",
              "    )\n",
              "    (thermal): Sequential(\n",
              "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
              "    )\n",
              "    (imu): Sequential(\n",
              "      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=512, out_features=1024, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (modality_postprocessors): ModuleDict(\n",
              "    (vision): Normalize()\n",
              "    (text): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
              "    )\n",
              "    (audio): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (depth): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (thermal): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (imu): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from imagebind.models import imagebind_model\n",
        "from imagebind.models.imagebind_model import ModalityType\n",
        "from imagebind.data import load_and_transform_audio_data\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "model.eval()\n",
        "model.to(device)\n"
      ],
      "id": "aJim9O4O19hv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNjNxxLs19hv",
        "outputId": "7f067e3f-7b30-4c41-866d-e6db65f66722"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/audio. Please upload your audio files here.\n",
            "Found audio files:\n",
            "No audio files found in /content/audio. Please upload your audio files (.wav, .mp3, .flac) to this directory.\n"
          ]
        }
      ],
      "source": [
        "# Load audio files from folder and compute embeddings\n",
        "audio_folder = \"/content/audio\"  # TODO: put your audio files here (or mount Google Drive)\n",
        "\n",
        "# Create the audio folder if it doesn't exist\n",
        "if not os.path.exists(audio_folder):\n",
        "    os.makedirs(audio_folder)\n",
        "    print(f\"Created directory: {audio_folder}. Please upload your audio files here.\")\n",
        "\n",
        "audio_paths = [\n",
        "    os.path.join(audio_folder, f)\n",
        "    for f in os.listdir(audio_folder)\n",
        "    if f.lower().endswith(('.wav', '.mp3', '.flac'))\n",
        "]\n",
        "\n",
        "print(\"Found audio files:\")\n",
        "for p in audio_paths:\n",
        "    print(p)\n",
        "\n",
        "# If no audio files are found, provide a message and exit gracefully\n",
        "if not audio_paths:\n",
        "    print(f\"No audio files found in {audio_folder}. Please upload your audio files (.wav, .mp3, .flac) to this directory.\")\n",
        "else:\n",
        "    audio_inputs = load_and_transform_audio_data(audio_paths, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings_dict = model({ModalityType.AUDIO: audio_inputs})\n",
        "    audio_embeddings = embeddings_dict[ModalityType.AUDIO].cpu().numpy()\n",
        "    print(\"Audio embeddings shape:\", audio_embeddings.shape)\n"
      ],
      "id": "dNjNxxLs19hv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "116febfa",
        "outputId": "4c59159f-4482-4c48-d371-a5ddcfc3c08f"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "audios_to_download = {\n",
        "    'sample_audio_1.mp3': 'https://file-examples.com/storage/fe39414f4963503b17c7625/2017/11/file_example_MP3_700KB.mp3',\n",
        "    'sample_audio_2.mp3': 'https://file-examples.com/storage/fe39414f4963503b17c7625/2017/11/file_example_MP3_1MG.mp3'\n",
        "}\n",
        "\n",
        "for filename, url in audios_to_download.items():\n",
        "    filepath = os.path.join(audio_folder, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status() # Raise an exception for bad status codes\n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"Downloaded {filename} to {filepath}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists at {filepath}\")\n",
        "\n",
        "# Re-run the audio file detection after downloading\n",
        "audio_paths = [\n",
        "    os.path.join(audio_folder, f)\n",
        "    for f in os.listdir(audio_folder)\n",
        "    if f.lower().endswith(('.wav', '.mp3', '.flac'))\n",
        "]\n",
        "\n",
        "print(\"Found audio files after download attempt:\")\n",
        "for p in audio_paths:\n",
        "    print(p)\n",
        "\n",
        "# Proceed with embeddings if files are found\n",
        "if audio_paths:\n",
        "    audio_inputs = load_and_transform_audio_data(audio_paths, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings_dict = model({ModalityType.AUDIO: audio_inputs})\n",
        "    audio_embeddings = embeddings_dict[ModalityType.AUDIO].cpu().numpy()\n",
        "    print(\"Audio embeddings shape:\", audio_embeddings.shape)\n",
        "else:\n",
        "    print(\"No audio files found even after download attempt. Please check the URLs or upload manually.\")"
      ],
      "id": "116febfa",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample_audio_1.mp3...\n",
            "Error downloading sample_audio_1.mp3: 403 Client Error: Forbidden for url: https://file-examples.com/storage/fe39414f4963503b17c7625/2017/11/file_example_MP3_700KB.mp3\n",
            "Downloading sample_audio_2.mp3...\n",
            "Error downloading sample_audio_2.mp3: 403 Client Error: Forbidden for url: https://file-examples.com/storage/fe39414f4963503b17c7625/2017/11/file_example_MP3_1MG.mp3\n",
            "Found audio files after download attempt:\n",
            "/content/audio/song1.mp3\n",
            "/content/audio/song16.mp3\n",
            "Audio embeddings shape: (2, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLfgulTD19hv",
        "outputId": "2d0693f6-2a7a-4557-afdd-7e26ff3e4fb9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score: nan\n",
            "Audio: song1.mp3 -> Cluster 0\n",
            "Audio: song16.mp3 -> Cluster 1\n"
          ]
        }
      ],
      "source": [
        "# Cluster audio embeddings & evaluate\n",
        "num_clusters = 2  # Adjusted to be less than or equal to the number of samples (2)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "labels = kmeans.fit_predict(audio_embeddings)\n",
        "\n",
        "# Calculate silhouette score only if it's meaningful (i.e., 1 < num_clusters < n_samples)\n",
        "sil = silhouette_score(audio_embeddings, labels) if (num_clusters > 1 and num_clusters < audio_embeddings.shape[0]) else float(\"nan\")\n",
        "print(\"Silhouette score:\", sil)\n",
        "\n",
        "for path, label in zip(audio_paths, labels):\n",
        "    print(f\"Audio: {os.path.basename(path)} -> Cluster {label}\")"
      ],
      "id": "VLfgulTD19hv"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}